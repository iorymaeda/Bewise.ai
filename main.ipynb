{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c841a01-c6a9-4e86-9e25-3bbaa81fe3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3372380e-4a1d-4098-af23-11be37c1119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\")\n",
    "model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased-conversational\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f0ad45-55ed-402e-adb3-1efa9e995bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pooling(window: torch.Tensor, seq_len: torch.Tensor, key_padding_mask: torch.Tensor)  -> torch.FloatTensor:\n",
    "    \"\"\"Помогает пулить padded последовательности\n",
    "    \n",
    "    source: https://github.com/iorymaeda/ArcDOTA/blob/master/utils/nn/prematch.py \n",
    "    \n",
    "    :param window: - window/array to pool\n",
    "    | window | : (batch_size, seq_len, d_model)\n",
    "    \n",
    "    :param seq_len: - array with seq_len per sample in batch\n",
    "    | seq_len | : (batch_size)\n",
    "    \n",
    "    :param key_padding_mask: - array with padded mask\n",
    "    | key_padding_mask | : (batch_size, seq_len)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # multiply window by mask - zeros all padded tokens\n",
    "    pooled = torch.mul(window, key_padding_mask.unsqueeze(2))\n",
    "    # |pooled| : (batch_size, seq_len, d_model)\n",
    "\n",
    "    # sum all elements by seq_len dim\n",
    "    pooled = pooled.sum(dim=1)\n",
    "    # |pooled| : (batch_size, d_model)\n",
    "\n",
    "    # divide samples by by its seq_len, so we will get mean values by each sample\n",
    "    pooled = pooled / seq_len.unsqueeze(1)\n",
    "    # |pooled| : (batch_size, d_model)\n",
    "\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4ede47-d10e-4753-b0d6-41951abf6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(text: str) -> str:\n",
    "    return text.replace(\"-\", \" \").replace(\"_\", \" \").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "182a83b7-a54c-4b81-8307-9d71a37fe184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет мир, меня', 'мир, меня зовут', 'меня зовут Ангелина']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crop_text_to_patches(text: str, slice_range:int=2) -> list:\n",
    "    query_slices = []\n",
    "    query_words = split(text)\n",
    "    \n",
    "    if len(query_words) < slice_range:\n",
    "        query_slices.append(text)\n",
    "        \n",
    "    else:\n",
    "        for batch in range(0, len(query_words)-(slice_range-1)):\n",
    "            s = \"\"\n",
    "            for _ in range(slice_range):\n",
    "                s+= query_words[batch + _] + \" \"\n",
    "            s = s.strip()\n",
    "\n",
    "            query_slices.append( s )\n",
    "            \n",
    "    return query_slices\n",
    "\n",
    "crop_text_to_patches('Привет мир, меня зовут Ангелина', slice_range=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f94de3-0907-4d30-a7eb-81ee0a6df2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_device(d, device):\n",
    "    return {k: v.to(device) for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175c2ec-38c5-4b2d-9b29-0dfabd1866fd",
   "metadata": {},
   "source": [
    "\\\\(\\mathbf {cos\\_similarity} =\\|\\mathbf {a} \\|\\ \\|\\mathbf {b} \\|\\cos \\theta \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83796a4b-5a9a-4263-be39-7c04cbe0d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def classify(query: str, anchor: list, slice_range=2) -> float:\n",
    "    query = query.lower()\n",
    "    query_slices = crop_text_to_patches(query, slice_range=slice_range)\n",
    "    \n",
    "    text = anchor + query_slices\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True)\n",
    "    out = model(**dict_to_device(tokens, device))\n",
    "    out = dict_to_device(out, 'cpu')\n",
    "    \n",
    "    # embs = out['last_hidden_state'].mean(dim=1)\n",
    "    embs = avg_pooling(out['last_hidden_state'], tokens['attention_mask'].sum(1), tokens['attention_mask'])\n",
    "    normed = embs / torch.norm(embs, dim=1, keepdim=True)\n",
    "    \n",
    "    _anchor, _query = normed[:len(anchor)], normed[len(anchor):]\n",
    "    max_ = (_anchor @ _query.T).max()\n",
    "    \n",
    "    if slice_range == 1:\n",
    "        return max_\n",
    "    else:\n",
    "        return max(max_, classify(query, anchor, slice_range-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d37628a-0c1b-4a7c-8f66-9986b1b0e031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ner Модель Не Умеет Работать С Именнованными Сущностями Начинающимися С Нижнего Регистра :('"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_camel_case(text):\n",
    "    if len(text) == 0: return text\n",
    "\n",
    "    s = split(text)\n",
    "    output = \"\"\n",
    "    for _t in s:\n",
    "        output += _t.capitalize()\n",
    "        output += \" \"\n",
    "    return output.rstrip()\n",
    "\n",
    "to_camel_case('NER модель не умеет работать с именнованными сущностями начинающимися с нижнего регистра :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87e3d0da-6ea5-4000-af02-b5190d759ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(replic: str) -> dict[str, list[int]]:\n",
    "    # Кропаем и подаём патчами т.к. модель глупенькая и на больших предложениях путаеться \n",
    "    return crop_text_to_patches(to_camel_case(replic), slice_range=5) \n",
    "\n",
    "def get_ner(replic: str): \n",
    "    ner = session.post(\n",
    "        url=\"http://127.0.0.1:8201/model\", \n",
    "        json={ \"x\": preproc(replic)}\n",
    "    ).json()\n",
    "    \n",
    "    # Куча постпроцессинга\n",
    "    ners = [[] for _ in range(len(split(replic)))]\n",
    "    words = []\n",
    "    for idx, v in enumerate(ner):\n",
    "        t, n = v\n",
    "\n",
    "        if idx == 0: words += t\n",
    "        else: words += t[-1:]\n",
    "\n",
    "        n = [re.sub(\"\\D-\", \"\", ner_v) for ner_v in n]\n",
    "        for jdx, ner_v in enumerate(n):\n",
    "            ners[idx+jdx].append(ner_v)\n",
    "\n",
    "    align = []\n",
    "    for i in ners:\n",
    "        d = {}\n",
    "        for entity in i:\n",
    "            if entity in d:\n",
    "                d[entity] += 1\n",
    "            else:\n",
    "                d[entity] = 1\n",
    "\n",
    "        for key in d:\n",
    "            d[key] /= len(i)\n",
    "        align.append(d)\n",
    "        \n",
    "    NER = []\n",
    "    for t in align:\n",
    "        entity = max(t, key=t.get)\n",
    "        if t[entity] >= 0.5:\n",
    "            NER.append(entity)\n",
    "        else:\n",
    "            NER.append('O')\n",
    "\n",
    "    collected = {}\n",
    "    _previous_num = 0\n",
    "    _previous_tag = NER[0]\n",
    "    for idx, tag in enumerate(NER):\n",
    "        if idx > 0:\n",
    "            if tag != _previous_tag:\n",
    "                if _previous_tag not in collected:\n",
    "                    collected[_previous_tag] = []\n",
    "\n",
    "                collected[_previous_tag].append([_previous_num, idx])\n",
    "\n",
    "                _previous_num = idx\n",
    "                _previous_tag = tag\n",
    "\n",
    "    if _previous_tag not in collected:\n",
    "        collected[_previous_tag] = []\n",
    "\n",
    "    collected[_previous_tag].append([_previous_num, len(NER)])\n",
    "    \n",
    "    if 'O' in collected:\n",
    "        del collected['O']\n",
    "    \n",
    "    return collected\n",
    "\n",
    "def extract_ner(replic: str):\n",
    "    ners = get_ner(replic)\n",
    "    splitted_replic = split(replic)\n",
    "    for key in ners:\n",
    "        for idx, pos in enumerate(ners[key]):\n",
    "            s = \"\"\n",
    "            _replic = splitted_replic[pos[0]:pos[1]]\n",
    "            for word in _replic:\n",
    "                s+= word\n",
    "                s+= \" \"\n",
    "            s = s.strip()\n",
    "\n",
    "            ners[key][idx] = s\n",
    "    return ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46046a45-6de7-40ea-804c-d179ab79726f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 480/480 [01:41<00:00,  4.71it/s]\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "df = pd.read_csv('test_data.csv')\n",
    "\n",
    "for idx, replic in enumerate(tqdm(df['text'])): \n",
    "    replic = replic.strip()\n",
    "    \n",
    "    # --------------------------------------------------------------------- #\n",
    "    ners = extract_ner(replic)\n",
    "    greetings = classify(replic, slice_range=2, anchor=['приветствие', 'здравствуйте', 'добрый день', 'доброе утро', 'добрый вечер'])\n",
    "    farewells = classify(replic, slice_range=2, anchor=['до свидания', 'всего хорошего', 'прощай', 'прощайте', 'хорошего дня', 'хорошего вечера'])\n",
    "    name = classify(replic, slice_range=3, anchor=['меня зовут', 'зовут меня'])\n",
    "\n",
    "    greetings = greetings.item()\n",
    "    farewells = farewells.item()\n",
    "    name = name.item()\n",
    "    \n",
    "    # --------------------------------------------------------------------- #\n",
    "    # speech2text съездает некоторые слова, отдельно обработаем такие случаи\n",
    "    words = split(replic.lower())[0]\n",
    "    if words[0] in ['алло', 'ало']: del words[0]\n",
    "    \n",
    "    if words[0] in ['добрый', 'доброе']:\n",
    "        greetings = 0.9 if greetings < 0.9 else greetings\n",
    "    \n",
    "    # --------------------------------------------------------------------- #\n",
    "    df.loc[idx, 'greetings_score'] = greetings\n",
    "    df.loc[idx, 'farewells_score'] = farewells\n",
    "    df.loc[idx, 'name_score'] = name\n",
    "\n",
    "    for entity in ners:\n",
    "        df.loc[idx, entity] = ners[entity][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d490168-7568-40e2-8806-aee524f6681f",
   "metadata": {},
   "source": [
    "### post proccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f9a19c8-f4f7-4e89-b07a-31be7ca79d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4b22966-b399-4452-923e-054b88cb3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63a7c008-aa04-40fb-9368-bcb2382b627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель очень тригерится на Алло, думает это имена\n",
    "# К сожалению скоры deeppavlov не выдаёт\n",
    "df.loc[df['PER'] == 'Алло', 'PER'] = float('nan')\n",
    "df[\"PER\"] = df[\"PER\"].apply(lambda x:  x.lower().replace(\"алло\", \"\") if type(x) is str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "77afe3ab-0527-4953-9e6d-50d7806e0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это нам не надо\n",
    "df.drop(['LOC'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "db722100-06c1-41e4-8e30-e40ccd7ec3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Что-то мне кажется что роль менеджера и клиента перепутаны\n",
    "# Буду считать всё для клиентов представляю что это менеджеры, к тому же этот нюанс можно уточнить и легко исправить\n",
    "\n",
    "# Ещё один пост процессинг, обычно здороваются в начале диалога, предпологаю что компанию тоже стоит назвать в начале диалога,\n",
    "# а прощаются в конце, таким образом отчистим от мусора который выдал deeppavlov\n",
    "role = 'client'\n",
    "greetings_index = []\n",
    "farewells_index = []\n",
    "name_index = []\n",
    "\n",
    "per_index = []\n",
    "org_index = []\n",
    "\n",
    "for dlg_id  in df['dlg_id'].unique():\n",
    "    corpus = df[(df['dlg_id'] == dlg_id) & (df['role'] == role)]\n",
    "    \n",
    "    # Приветсвие обычно в начале\n",
    "    slice_ = corpus.iloc[:2]['greetings_score']\n",
    "    slice_ = slice_[slice_ >= 0.85]\n",
    "    for index in slice_[slice_ == slice_.max()].index:\n",
    "        greetings_index.append(index)\n",
    "        break\n",
    "        \n",
    "    # Прощание обыччно в конце\n",
    "    slice_ = corpus.iloc[-4:]['farewells_score']\n",
    "    slice_ = slice_[slice_ >= 0.85]\n",
    "    for index in slice_[slice_ == slice_.max()].index:\n",
    "        farewells_index.append(index)\n",
    "        break\n",
    "        \n",
    "    slice_ = corpus.iloc[:4]['name_score']\n",
    "    slice_ = slice_[slice_ >= 0.8]\n",
    "    for index in slice_[slice_ == slice_.max()].index:\n",
    "        name_index.append(index)\n",
    "        break\n",
    "        \n",
    "    slice_ = corpus.iloc[:4]['PER']\n",
    "    for index in slice_.index:\n",
    "        per_index.append(index)\n",
    "    \n",
    "    slice_ = corpus.iloc[:4]['ORG']\n",
    "    for index in slice_.index:\n",
    "        org_index.append(index)\n",
    "        \n",
    "df['greetings'] = False\n",
    "df['farewells'] = False\n",
    "df['name'] = False\n",
    "df.loc[greetings_index, 'greetings'] = True\n",
    "df.loc[farewells_index, 'farewells'] = True\n",
    "df.loc[name_index, 'name'] = True\n",
    "\n",
    "df['PER_name'] = float('nan')\n",
    "df['ORG_name'] = float('nan')\n",
    "df.loc[per_index, 'PER_name'] = df.loc[per_index, 'PER']\n",
    "df.loc[org_index, 'ORG_name'] = df.loc[org_index, 'ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0bd15f59-6537-4629-b210-d7570a3eea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем в словарь всё остальное\n",
    "\n",
    "dlf_output = {}\n",
    "for dlg_id  in df['dlg_id'].unique():\n",
    "    df_slice = df[(df['dlg_id'] == dlg_id)]\n",
    "    \n",
    "    greeting = df_slice['greetings'].any()\n",
    "    farewell = df_slice['farewells'].any()\n",
    "    \n",
    "    PER_name = df_slice[df_slice['role'] == role]['PER_name'].dropna().unique().tolist()\n",
    "    ORG_name = df_slice[df_slice['role'] == role]['ORG_name'].dropna().unique().tolist()\n",
    "    \n",
    "    greeting_text = df_slice[df_slice['greetings']]['text']\n",
    "    farewell_text = df_slice[df_slice['farewells']]['text']\n",
    "    name_text = df_slice[df_slice['name']]['text']\n",
    "    \n",
    "    greeting_text = greeting_text.values[0] if len(greeting_text) else ''\n",
    "    farewell_text = farewell_text.values[0] if len(farewell_text) else ''\n",
    "    name_text = name_text.values[0] if len(name_text) else ''\n",
    "    \n",
    "    dlf_output[dlg_id] = {\n",
    "        'greeting': greeting,\n",
    "        'farewell': farewell,\n",
    "        'farewell_text': farewell_text,\n",
    "        'greeting_text': greeting_text,\n",
    "        'name_text': name_text,\n",
    "        'PER_name': PER_name,\n",
    "        'ORG_name': ORG_name,\n",
    "        'is_polite': (greeting and farewell)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c522530-950e-4289-bf60-2b63b9121c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'greeting': True,\n",
       "  'farewell': True,\n",
       "  'farewell_text': 'Всего хорошего до свидания',\n",
       "  'greeting_text': 'Алло здравствуйте',\n",
       "  'name_text': 'Меня зовут ангелина компания диджитал бизнес звоним вам по поводу продления лицензии а мы с серым у вас скоро срок заканчивается',\n",
       "  'PER_name': ['ангелина'],\n",
       "  'ORG_name': ['компания диджитал бизнес'],\n",
       "  'is_polite': True},\n",
       " 1: {'greeting': True,\n",
       "  'farewell': True,\n",
       "  'farewell_text': 'До свидания',\n",
       "  'greeting_text': 'Алло здравствуйте',\n",
       "  'name_text': 'Меня зовут ангелина компания диджитал бизнес звоню вам по поводу продления а мы сели обратила внимание что у вас срок заканчивается',\n",
       "  'PER_name': ['ангелина'],\n",
       "  'ORG_name': ['компания диджитал бизнес'],\n",
       "  'is_polite': True},\n",
       " 2: {'greeting': True,\n",
       "  'farewell': False,\n",
       "  'farewell_text': '',\n",
       "  'greeting_text': 'Алло здравствуйте',\n",
       "  'name_text': 'Меня зовут ангелина компания диджитал бизнес звоню вам по поводу продления лицензии а мастера мы с вами сотрудничали по видео там',\n",
       "  'PER_name': ['ангелина'],\n",
       "  'ORG_name': ['компания диджитал бизнес'],\n",
       "  'is_polite': False},\n",
       " 3: {'greeting': True,\n",
       "  'farewell': True,\n",
       "  'farewell_text': 'Угу все хорошо да понедельника тогда всего доброго',\n",
       "  'greeting_text': 'Алло дмитрий добрый день',\n",
       "  'name_text': 'Добрый меня максим зовут компания китобизнес удобно говорить',\n",
       "  'PER_name': [' дмитрий', 'максим зовут', 'дмитрий', 'угу'],\n",
       "  'ORG_name': ['китобизнес'],\n",
       "  'is_polite': True},\n",
       " 4: {'greeting': False,\n",
       "  'farewell': True,\n",
       "  'farewell_text': 'Во вторник все ну с вами да тогда до вторника до свидания',\n",
       "  'greeting_text': '',\n",
       "  'name_text': '',\n",
       "  'PER_name': [],\n",
       "  'ORG_name': [],\n",
       "  'is_polite': False},\n",
       " 5: {'greeting': False,\n",
       "  'farewell': True,\n",
       "  'farewell_text': 'Ну до свидания хорошего вечера',\n",
       "  'greeting_text': '',\n",
       "  'name_text': '',\n",
       "  'PER_name': ['анастасия', 'угу'],\n",
       "  'ORG_name': [],\n",
       "  'is_polite': False}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbaf70e-27b6-4eb9-9507-ba923ee45962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
